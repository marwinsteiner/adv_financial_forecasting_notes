# Week 2 Lecture Notes

## Smoothing

- Not the most sophisticated methodology
- But a useful benchmark: if you don't beat smoothing, there's something very wrong with your model.
- And, in some cases, smoothing may work, although we tend to think that it is superseded by ARMA models, with which you
  can see some similarities

## Big Idea

- Any time series $y$ can be decomposed as having
    - Some level of the series, $L$
    - A trend in the series, $T$
    - A seasonal or other cycle in the series, $S$
    - And some random noisem $u$
- Time series forecasting techniques ai to forecast y by inferring L, T, and S from past observations on $y$
- Exact method depends on assumptions about time series processes.

## Cases/Methods

### Case 1: No Trends in Data

- Suppose we
    - assume that data has no trend
    - observe data $y_0, y_1, y_3, \text{...}, y_{t-1}$
    - make an estimate of the level $L_{t-1}$ of the series at $t-1$
- Then $y_{t-1} = L_{t-1} + u_{t-1}$
- When we get new observation $y_t$ we revise our estimate of the level up or down, as
  $$L_t = \alpha y_t + (1- \alpha) L_{t-1}, \space 0 \leq a \leq 1$$
- The rule $L_t = \alpha y_t + (1- \alpha) L_{t-1}$
    - is the recursive form of the single exponential smoothing model whereby $\alpha$ is the "smoothing constant"
      and $0 \leq a \leq 1$
    - Forecast of $y$ at future times $t+h, h \in \{1, 2, 3, \text{...}\}$ is constant at $L_t$
      $$F_{t+h} = L_t$$

### Case 2: Linear Trends

- Suppose we
    - assume that data has linear trend $T$ per period
    - observe data $y_0, y_1, y_3, \text{...}, y_{t-1}$
    - make estimate of the level $L_{t-1}$ of the series at $t-1$
    - When we get new observation $y_t$, we revise our estimate of the level and trend up or down as follows
      $$L_t = \alpha y_t + (1- \alpha)(L_{t-1} + T_{t-1}), \space 0 \leq a \leq 1$$
      $$T_t = \gamma (L_t - L_{t-1}) + (1-\gamma)T_{t-1}$$
      These two recursive relations are the Holt linear trend model.
- Forecasts for periods $t+1, t+2, t+3, \text{...}, t+h$ are generated by taking the current level estimate, and
  adding $h$ times the current trend estimate
  $$y_{t+h}* = L_t + h \cdot T_t$$

### Case 3: Trends and Seasonality

- Suppose we
    - Assume that data has linear trend $T$ per period
    - Assume data has $k$-period seasonality $S$
    - Observe data $y_0, y_1, y_3, \text{...}, y_{t-1}$ up to time $t-1$
    - When we get new observation $y_t$, we revise our estimate of the level, trend, and relevant seasonal factor up or
      down:
      $$\begin{align}
      L_t &= \alpha \frac{y_t}{S_{t-k}} + (1-\alpha)(L_{t-1} + T_{t-1}) \\
      T_t &= \gamma (L_t - L_{t-1}) + (1-\gamma)T_{t-1} \\
      S_t &= \delta (\frac{y_t}{L_t}) + (1-\delta)S_{t-k}
      \end{align}$$

- Seasonality: $k$ is period of seasonality (e.g. 4 for quarterly data, 12 for monthly, 5 or 7 for weekly)
- Seasonal factors: $S_t$ is the ratio of the value of $y_t$ for that "season" to the underlying level.
- Seasonal adjustment: the series $y_t * = \frac{y_t}{S_t}$ is a "seasonally adjusted" time series for the variable $y$.
- Forecasts: given data up to $t$, forecasts for $t+1, t+2, t+3, \text{...}, t+h$ are
  $$F_{t+h} = (L_t + h \cdot T_t)S_{t-k+h}$$

## Time Series Analysis

### ARMA Models

#### Notation and Concepts

A strictly stationary process is one where
$$P\{y_{t_1} \leq b_1, \text{...} y_{t_n} \leq b_n\} = P\{y_{t_1 + m} \leq b_1 \text{...}, y_{t_n +m} \leq b_n\}$$
i.e. the probability measure for the sequence $\{y_t\}$ is the same as that for $\{y_{t+m}\} \forall m$

A weakly stationary process is one where

1. $E(y_t) = \mu, \space t=1, 2, 3, \space \text{...} \space \infty$
2. $E(y_{t_1} - \mu)(E(y_{t_2} - \mu) = \gamma_{t_1 - t_1} \space \forall \space  t_1, t_2$
3. $E(y_t - \mu)(y_t - \mu) = \sigma^2 < \infty$

#### Autocovariances

So if the process is covariance stationary, all the variances are the same and all the covariances depend on the
difference between $t_1$ and $t_2$. The moments $E(y_t - E(y_t))(y_{t+s} - E(y_{t+s})) = \gamma_s$
where $-\infty < s < \infty$ are known as the covariance function.

The covariances, $\gamma_s$ are known as autocovariances. However, the autocovariances depend on the units of
measurement of $y_t$.

It is thus more convenient to use the autocorrelation which are the autocovariances normalized by dividing by the
variance:
$$\tau_s = \frac{\gamma_s}{\gamma_0}, \text{where} \space -\infty < s < \infty$$
If we plot $\tau_s$ against $s=0, 1, 2, \space \text{...}$ then we obtain the autocorrelation function or correlogram.

#### A White Noise Process

A white noise process is one with (virtually) no discernible structure. A definition of a white noise process is
$$\begin{align}
E(y_t) &= \mu \\
Var(y_t) &= \sigma^2 \\
\gamma_{y_t-r} &= \begin{cases}
&\sigma^2 &\text{if} \space t=r \\
&0 &\text{otherwise}
\end{cases}
\end{align}$$

Thus, the autocorrelation function will be zero apart from a single peak of $1$ at $s=0$. Approximately, we can say
that $\tau_s ~ N(0, \frac{1}{T})$ where $T$ is the sample size.

We can use this to do significance tests for the autocorrelation coefficients by constructing a confidence interval.

For example, a 95% confidence interval would be given by $\pm \frac{1.96}{\sqrt{T}}$.
If the sample autocorrelation coefficient, $\hat{\tau_s}$, falls outside this region for any value of $s$, then we
reject the null hypothesis that the true value of the coefficient at lag $s$ is zero.

#### Joint Hypothesis Tests

We can also test the join hypothesis that all $m$ of the $\tau_k$ correlation coefficients are simultaneously equal to
zero using the Q-statistic developed by Box and Pierce:
$$Q = T\sum_{k=1}^m\tau_k^2$$
The Q-statistic is asymptotically distributed as a $\chi_{m}^2$.
However, the Box-Pierce test has poor small sample properties, so a variant has been developed, called the Ljung-Box statistic:
$$Q* = T(T+2)\sum_{k=1}^m\frac{\tau_k^2}{T-k} \text{~} \chi_{m}^2$$
This statistic is very useful as a test of linear dependence in time series. It can also be used on residuals.

## Bits from the Lecture
If your alpha is estimated based on historical data to estimate the historical interdependence on past data, isn't this
dangerous if you want to use the model for forecasting, because one of the assumptions you place on your model, is that
historical observations are a good predictor of the future? Wouldn't a user of the model care more about whether-or-not
the future is better or worse than the past?

Would you use a confidence interval for estimating alpha? What about building

This observation is relevant for all cases given, from 22-27.

This relates to slide 22.

Hamilton proof, chapter 2 relates to page 25 of the LN.

For exam preparation: we need to know the definition of covariance stationarity. She will ask for the definition and ask
an example. See slide 30 in the LN. The majority of models we will use require a weakly stationary process.